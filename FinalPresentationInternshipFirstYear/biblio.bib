@inproceedings{C_bound, 
    Publisher = {ACM}, 
    Title = {From PAC-Bayes Bounds to Quadratic Programs for Majority Votes }, 
    Url = {http://www.icml-2011.org/papers/379_icmlpaper.pdf}, 
    Booktitle = {Proceedings of the 28th International Conference on Machine Learning (ICML-11)}, 
    Author = {Jean-francis Roy and Mario Marchand and François Laviolette}, 
    Year = {2011}, 
    Editor = {Lise Getoor and Tobias Scheffer}, 
    Address = {New York, NY, USA}, 
    Pages = {649--656} 
   }
@article {Visual,
author = {House, Leanna and Leman, Scotland and Han, Chao},
title = {Bayesian visual analytics: BaVA},
journal = {Statistical Analysis and Data Mining},
volume = {8},
number = {1},
publisher = {Wiley Subscription Services, Inc., A Wiley Company},
issn = {1932-1872},
url = {http://dx.doi.org/10.1002/sam.11253},
doi = {10.1002/sam.11253},
pages = {1--13},
keywords = {Bayesian, visual analytics, elicitation, sequential updating, Ssense-making, data mining, high-dimensional data, statistical visualization},
year = {2015},
}

@ARTICLE{kano, 
author={A. Cano and A. R. Masegosa and S. Moral}, 
journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)}, 
title={A Method for Integrating Expert Knowledge When Learning Bayesian Networks From Data}, 
year={2011}, 
volume={41}, 
number={5}, 
pages={1382-1394}, 
keywords={Bayes methods;Monte Carlo methods;belief networks;graph theory;knowledge acquisition;learning (artificial intelligence);statistical distributions;Bayesian networks;Bayesian statistics;Monte Carlo simulations;automatic learning methods;expert knowledge integration;graph structures;knowledge elicitation;probability distributions;random variables;Approximation methods;Bayesian methods;Computational modeling;Data models;Markov processes;Monte Carlo methods;Uncertainty;Bayesian networks (BNs);Monte Carlo (MC) simulations;expert knowledge;interactive learning;probabilistic graphical models}, 
doi={10.1109/TSMCB.2011.2148197}, 
ISSN={1083-4419}, 
month={Oct},}
@Inbook{Loh2012,
author="Loh, Wei-Yin",
editor="Barbour, Andrew
and Chan, Hock Peng
and Siegmund, David",
title="Variable Selection for Classification and Regression in Large p, Small n Problems",
bookTitle="Probability Approximations and Beyond",
year="2012",
publisher="Springer New York",
address="New York, NY",
pages="135--159",
isbn="978-1-4614-1966-2",
doi="10.1007/978-1-4614-1966-2_10",
url="http://dx.doi.org/10.1007/978-1-4614-1966-2_10"
}
@article{GUIDE,
  title={Classification and regression trees},
  author={Loh, Wei-Yin},
  journal={Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  volume={1},
  number={1},
  pages={14--23},
  year={2011},
  publisher={Wiley Online Library}
}
@InProceedings{CQ_boost,
  title =    {A Column Generation Bound Minimization Approach with PAC-Bayesian Generalization Guarantees},
  author =   {Jean-Francis Roy and Mario Marchand and François Laviolette},
  booktitle =    {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  pages =    {1241--1249},
  year =     {2016},
  editor =   {Arthur Gretton and Christian C. Robert},
  volume =   {51},
  series =   {Proceedings of Machine Learning Research},
  address =      {Cadiz, Spain},
  month =    {09--11 May},
  publisher =    {PMLR},
  pdf =      {http://proceedings.mlr.press/v51/roy16.pdf},
  url =      {http://proceedings.mlr.press/v51/roy16.html},
  abstract =     {The C-bound, introduced in Lacasse et al (2006), gives a tight upper bound on the risk of the majority vote classifier. Laviolette et al. (2011) designed a learning algorithm named MinCq that outputs a dense distribution on a finite set of base classifiers by minimizing the C-bound, together with a PAC-Bayesian generalization guarantee. In this work, we design a column generation algorithm that we call CqBoost, that optimizes the C-bound and outputs a sparse distribution on a possibly infinite set of voters. We also propose a PAC-Bayesian bound for CqBoost that holds for finite and two cases of continuous sets of base classifiers. Finally, compare the accuracy and the sparsity of CqBoost with MinCq and other state-of-the-art boosting algorithms.}
}

@article{Knowledge_elicit,
  author    = {Pedram Daee and
               Tomi Peltola and
               Marta Soare and
               Samuel Kaski},
  title     = {Knowledge Elicitation via Sequential Probabilistic Inference for High-Dimensional
               Prediction},
  journal   = {CoRR},
  volume    = {abs/1612.03328},
  year      = {2016},
  url       = {http://arxiv.org/abs/1612.03328},
  timestamp = {Mon, 02 Jan 2017 11:09:15 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/DaeePSK16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@article{Lobato2015,
author="Hern{\'a}ndez-Lobato, Jos{\'e} Miguel
and Hern{\'a}ndez-Lobato, Daniel
and Su{\'a}rez, Alberto",
title="Expectation propagation in linear regression models with spike-and-slab priors",
journal="Machine Learning",
year="2015",
volume="99",
number="3",
pages="437--487",
abstract="An expectation propagation (EP) algorithm is proposed for approximate inference in linear regression models with spike-and-slab priors. This EP method is applied to regression tasks in which the number of training instances is small and the number of dimensions of the feature space is large. The problems analyzed include the reconstruction of genetic networks, the recovery of sparse signals, the prediction of user sentiment from customer-written reviews and the analysis of biscuit dough constituents from NIR spectra. The proposed EP method outperforms in most of these tasks another EP method that ignores correlations in the posterior and a variational Bayes technique for approximate inference. Additionally, the solutions generated by EP are very close to those given by Gibbs sampling, which can be taken as the gold standard but can be much more computationally expensive. In the tasks analyzed, spike-and-slab priors generally outperform other sparsifying priors, such as Laplace, Student's                                                                           {\$}{\$}t{\$}{\$}                                                            t                                                       and horseshoe priors. The key to the improved predictions with respect to Laplace and Student's                                                                           {\$}{\$}t{\$}{\$}                                                            t                                                       priors is the superior selective shrinkage capacity of the spike-and-slab prior distribution.",
issn="1573-0565",
doi="10.1007/s10994-014-5475-7",
url="http://dx.doi.org/10.1007/s10994-014-5475-7"
}

@article {Donoho,
	author = {Donoho, David and Tanner, Jared},
	title = {Observed universality of phase transitions in high-dimensional geometry, with implications for modern data analysis and signal processing},
	volume = {367},
	number = {1906},
	pages = {4273--4293},
	year = {2009},
	doi = {10.1098/rsta.2009.0152},
	publisher = {The Royal Society},
	abstract = {We review connections between phase transitions in high-dimensional combinatorial geometry and phase transitions occurring in modern high-dimensional data analysis and signal processing. In data analysis, such transitions arise as abrupt breakdown of linear model selection, robust data fitting or compressed sensing reconstructions, when the complexity of the model or the number of outliers increases beyond a threshold. In combinatorial geometry, these transitions appear as abrupt changes in the properties of face counts of convex polytopes when the dimensions are varied. The thresholds in these very different problems appear in the same critical locations after appropriate calibration of variables. These thresholds are important in each subject area: for linear modelling, they place hard limits on the degree to which the now ubiquitous high-throughput data analysis can be successful; for robustness, they place hard limits on the degree to which standard robust fitting methods can tolerate outliers before breaking down; for compressed sensing, they define the sharp boundary of the undersampling/sparsity trade-off curve in undersampling theorems. Existing derivations of phase transitions in combinatorial geometry assume that the underlying matrices have independent and identically distributed Gaussian elements. In applications, however, it often seems that Gaussianity is not required. We conducted an extensive computational experiment and formal inferential analysis to test the hypothesis that these phase transitions are universal across a range of underlying matrix ensembles. We ran millions of linear programs using random matrices spanning several matrix ensembles and problem sizes; visually, the empirical phase transitions do not depend on the ensemble, and they agree extremely well with the asymptotic theory assuming Gaussianity. Careful statistical analysis reveals discrepancies that can be explained as transient terms, decaying with problem size. The experimental results are thus consistent with an asymptotic large-n universality across matrix ensembles; finite-sample universality can be rejected. {\textcopyright} 2009 The Royal Society},
	issn = {1364-503X},
	URL = {http://rsta.royalsocietypublishing.org/content/367/1906/4273},
	eprint = {http://rsta.royalsocietypublishing.org/content/367/1906/4273.full.pdf},
	journal = {Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences}
}
@INPROCEEDINGS{Blitzer,
    author = {John Blitzer and Mark Dredze and Fernando Pereira},
    title = {Biographies, bollywood, boomboxes and blenders: Domain adaptation for sentiment classification},
    booktitle = {In ACL},
    year = {2007},
    pages = {187--205}
}
@article{Seeger,
 author = {Seeger, Matthias W.},
 title = {Bayesian Inference and Optimal Design for the Sparse Linear Model},
 journal = {J. Mach. Learn. Res.},
 issue_date = {6/1/2008},
 volume = {9},
 month = jun,
 year = {2008},
 issn = {1532-4435},
 pages = {759--813},
 numpages = {55},
 url = {http://dl.acm.org/citation.cfm?id=1390681.1390707},
 acmid = {1390707},
 publisher = {JMLR.org},
} 
@article{uncertain,
           title = {Uncertain Judgements: Eliciting Expert Probabilities},
          author = {A. O'Hagan and C. E. Buck and A. Daneshkhah and J. R. Eiser and P. H. Garthwaite and D. J. Jenkinson and J. E. Oakley and T. Rakow},
         address = {Chichester},
       publisher = {John Wiley},
            year = {2006},
             url = {http://oro.open.ac.uk/17948/},
        abstract = {Elicitation is the process of extracting expert knowledge about some unknown quantity or quantities, and formulating that information as a probability distribution. Elicitation is important in situations, such as modelling the safety of nuclear installations or assessing the risk of terrorist attacks, where expert knowledge is essentially the only source of good information. It also plays a major role in other contexts by augmenting scarce observational data, through the use of Bayesian statistical methods. However, elicitation is not a simple task, and practitioners need to be aware of a wide range of research findings in order to elicit expert judgements accurately and reliably. Uncertain Judgements introduces the area, before guiding the reader through the study of appropriate elicitation methods, illustrated by a variety of multi-disciplinary examples. }
}
@article{quantify,
          volume = {50},
          number = {3},
           title = {Quantifying expert opinion in linear regression problems},
          author = {Paul H. Garthwaite and James M. Dickey},
            year = {1988},
           pages = {462--474},
         journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
        keywords = {Bayesian regression analysis; elicitation tasks; normal linear model; prior assessment; prior distributions; probability assessment; probability elicitation
},
             url = {http://oro.open.ac.uk/17938/},
        abstract = {This paper describes a method for choosing a natural conjugate prior distribution for a normal linear sampling model. A person using the method to quantify his/her opinions performs specified elicitation tasks. The hyperparameters of the conjugate distribution are estimated from the elicited values. The method is designed to require elicitation tasks that people can perform competently and introduces a type of task not previously reported. A property of the method is that the assessed variance matrices are certain to be positive definite. The method is sufficiently simple to implement with an interactive computer program on a microcomputer.}
}
@article{interactive,
author = { Joseph B.   Kadane  and  James M.   Dickey  and  Robert L.   Winkler  and  Wayne S.   Smith  and  Stephen C.   Peters },
title = {Interactive Elicitation of Opinion for a Normal Linear Model},
journal = {Journal of the American Statistical Association},
volume = {75},
number = {372},
pages = {845-854},
year = {1980},
doi = {10.1080/01621459.1980.10477562},

URL = { 
        http://www.tandfonline.com/doi/abs/10.1080/01621459.1980.10477562
    
},
eprint = { 
        http://www.tandfonline.com/doi/pdf/10.1080/01621459.1980.10477562
    
}

}
@article{Vapnik_akshay,
  title={A new learning paradigm: Learning using privileged information},
  author={Vladimir Vapnik and Akshay Vashist},
  journal={Neural networks : the official journal of the International Neural Network Society},
  year={2009},
  volume={22 5-6},
  pages={544-57}
}
@article{variable,
title = "Variable selection via Gibbs sampling",
abstract = "A crucial problem in building a multiple regression model is the selection of predictors to include. The main thrust of this article is to propose and develop a procedure that uses probabilistic considerations for selecting promising subsets. This procedure entails embedding the regression setup in a hierarchical normal mixture model where latent variables are used to identify subset choices. In this framework the promising subsets of predictors can be identified as those with higher posterior probability. The computational burden is then alleviated by using the Gibbs sampler to indirectly sample from this multinomial posterior distribution on the set of possible subset choices. Those subsets with higher probability—the promising ones—can then be identified by their more frequent appearance in the Gibbs sample. © 1993 Taylor & Francis Group, LLC.",
keywords = "Data augmentation, Hierarchical Bayes, Latent variables, Mixture, Multiple regression",
author = "George, {Edward I.} and McCulloch, {Robert E.}",
year = "1993",
doi = "10.1080/01621459.1993.10476353",
volume = "88",
pages = "881--889",
journal = "Journal of the American Statistical Association",
issn = "0162-1459",
publisher = "Taylor and Francis Ltd.",
number = "423",
}
@ARTICLE{bayesian,
title = {Bayesian nonlinear regression for large p small n problems},
author = {Chakraborty, Sounak and Ghosh, Malay and Mallick, Bani K.},
year = {2012},
journal = {Journal of Multivariate Analysis},
volume = {108},
number = {C},
pages = {28-40},
abstract = {Statistical modeling and inference problems with sample sizes substantially smaller than the number of available covariates are challenging. This is known as large p small n problem. Furthermore, the problem is more complicated when we have multiple correlated responses. We develop multivariate nonlinear regression models in this setup for accurate prediction. In this paper, we introduce a full Bayesian support vector regression model with Vapnik’s ϵ-insensitive loss function, based on reproducing kernel Hilbert spaces (RKHS) under the multivariate correlated response setup. This provides a full probabilistic description of support vector machine (SVM) rather than an algorithm for fitting purposes. We have also introduced a multivariate version of the relevance vector machine (RVM). Instead of the original treatment of the RVM relying on the use of type II maximum likelihood estimates of the hyper-parameters, we put a prior on the hyper-parameters and use Markov chain Monte Carlo technique for computation. We have also proposed an empirical Bayes method for our RVM and SVM. Our methods are illustrated with a prediction problem in the near-infrared (NIR) spectroscopy. A simulation study is also undertaken to check the prediction accuracy of our models.},
keywords = {Bayesian hierarchical model; Empirical Bayes; Gibbs sampling; Markov chain Monte Carlo; Metropolis–Hastings algorithm; Near infrared spectroscopy; Relevance vector machine; Reproducing kernel Hilbert space; Support vector machine; Vapnik’s ϵ-insensitive loss;},
url = {http://EconPapers.repec.org/RePEc:eee:jmvana:v:108:y:2012:i:c:p:28-40}
}
@techreport{interactive,
  title={Interactive Algorithms for Unsupervised Machine Learning},
  author={Krishnamurthy, Akshay},
  year={2015},
  institution={CARNEGIE-MELLON UNIV PITTSBURGH PA SCHOOL OF COMPUTER SCIENCE}
}
@inproceedings{expectation,
 author = {Heskes, Tom and Zoeter, Onno},
 title = {Expectation Propagation for Approximate Inference in Dynamic Bayesian Networks},
 booktitle = {Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence},
 series = {UAI'02},
 year = {2002},
 isbn = {1-55860-897-4},
 location = {Alberta, Canada},
 pages = {216--223},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=2073876.2073902},
 acmid = {2073902},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 
@inproceedings{diefenbach2017trill,
  title={Trill: A reusable front-end for qa systems},
  author={Diefenbach, Dennis and Amjad, Shanzay and Both, Andreas and Singh, Kamal and Maret, Pierre},
  booktitle={European Semantic Web Conference},
  pages={48--53},
  year={2017},
  organization={Springer}
}
@article{berners2001semantic,
  title={The semantic web},
  author={Berners-Lee, Tim and Hendler, James and Lassila, Ora},
  journal={Scientific american},
  volume={284},
  number={5},
  pages={34--43},
  year={2001},
  publisher={JSTOR}
}
@misc{dennis,
 	author       = {Dennis Diefenbach},
 	title        = {},
 	year         = {},
 	howpublished = {\url{https://qanswer-frontend.univ-st-etienne.fr/faq}},
  	note         = {}
  }
@misc{wiki,
 	author       = {},
 	title        = {},
 	year         = {},
 	howpublished = {\url{https://en.wikipedia.org/wiki/SPARQL}},
  	note         = {}
  }

@misc{QvsS,
 	author       = { Mark Baker},
 	title        = {Search vs. Query
},
 	year         = {},
 	howpublished = {\url{https://rb.gy/gba3ev}},
  	note         = {}
  }

@misc{Quora,
 	author       ={Nitin Thokare},
 	title        = {How does a Google image search engine work
},
 	year         = {2013},
 	howpublished = {\url{https://rb.gy/mb2vqp}},
  	note         = {}
  }
  
  @misc{wikimedia,
 	author       ={},
 	title        = {api parameters
},
 	year         = {},
 	howpublished = {\url{https://www.mediawiki.org/wiki/API:Search}},
  	note         = {}
  }

  @misc{class,
 	author       ={},
 	title        = {class number and name
},
 	year         = {},
 	howpublished = {\url{https://rb.gy/ofdhb0}},
  	note         = {}
  }
  
  @misc{imageRecognitionIntro,
 	author       ={},
 	title        = {8 Uses Cases Of Image Recognition That We See In Our Daily Lives
},
 	year         = {},
 	howpublished = {\url{https://rb.gy/3bf7hi}},
  	note         = {}
  }

  @misc{imageRecognitionDef,
 	author       ={},
 	title        = {Recognition methods in image processing
},
 	year         = {},
 	howpublished = {\url{https://rb.gy/mvezbh}},
  	note         = {}
  }
  
    @misc{imageRecognitionDetails,
 	author       ={},
 	title        = {What Is Image Recognition
},
 	year         = {},
 	howpublished = {\url{https://rb.gy/gaxy5m}},
  	note         = {}
  }
  
 @Misc{ich18,	
	author              = {},
	title               = {{ICH Multi-annual strategic plan}},
	year                = {2018},	
	howpublished        = {\url{https://goo.gl/AnR5hs}},	
	note                = {Accessed March 22, 2018}
 }


 @Misc{ncrb15,	
	author              = {{NCRB Report}},
	title               = {Accidental deaths \& suicides in {I}ndia {(ADSI)}},
	year                = {2015},	
	howpublished        = {\url{http://ncrb.gov.in/}},
	note                = {Accessed April 06, 2017},		
 }


 @Misc{suc19,	
 	author               = {},
 	title                = {{Table-sucrose solutions, composition, viscosity, 
 	                       density \@ 20 $^\circ${C}}},	
 	year                 = {},
	howpublished         = {\url{http://lclane.net/text/sucrose.html}},
	note                 = {Accessed January 30, 2018},	
 }







